- 2022-01-17
- status: proposal

## A foundation for building font tooling in Rust


# Overview

We propose a Rust library (crate) for zero-copy access and direct manipulation
of font tables and types, leveraging Rust’s procedural macro system. This
library is intended to be a zero-opinion transparent representation of the types
in the specification, to serve as a foundational component for anything that
needs to read or write font files. An additional goal is to provide a common set
of types to represent common data formats (tags, glyph identifiers, etc) that
can be used throughout the broader ecosystem.


# Motivation

This is intended to answer some of the questions outlined in this repository's
README. In particular, we aim to determine whether the following goals are feasible:

- using macros to describe the shape of tables
- generating both zero-copy read-only table definitions as well as owned,
  mutable definitions (for compilation)
- suitable for prototyping & extensibility, i.e. it should be easy to add a new
  table type for a project without needing to patch the core crate or maintain a
  fork.


# Design

The particulars of the design will be determined through this exploration, but a
few core ideas are clear.


## Procedural Macros

The crate will rely heavily on procedural macros to generate the code for
representing and accessing font tables. As an illustration, defining a simple
table might look something like,

```rust
#[derive(Debug, OpenTypes)]
struct Avar<'a> {
    // code to convert from raw bytes is generated by macro based on scalar type
    major_version: u16,
    minor_version: u16,
    #[opentypes(padding(u16))]
    reserved: (),
    #[opentypes(counted)]
    segment_maps: Slice<'a, SegmentMaps<'a>>,
    data: Buffer<'a>,
}

#[derive(Debug, OpenTypes)]
struct SegmentMaps<'a> {
    #[opentypes(counted)]
    axis_value_maps: Slice<'a, AxisValueMap>,
    data: Buffer<'a>,
}

#[derive(Debug, Clone, OpenTypes)]
struct AxisValueMap {
    from_coordinate: F2dot14,
    to_coordinate: F2dot14,
}
```

Although it's possible that `derive` is not the right model here, and perhaps we
don't want to actually have any fields at all, and all access is through
generated methods? In which case the generated code would look more like,

```rust
describe_shape!(Avar {
    version_major: u16,
    version_minor: u16,
    Padding(u16),
    CountedSlice(u16, SegmentMaps),
});

struct Avar<'a>(Buffer<'a>);

impl<'a> Avar<'a> {
    fn version_major(&self) -> Result<u16, Error> {
        self.buffer.read(0)
    }

    fn version_major(&self) -> Result<u16, Error> {
        self.buffer.read(2)
    }

    fn segment_maps(&self) -> Result<Slice<'a, SegmentMaps>> {
        let len: u16 = self.buffer.read(6);
        self.buffer.slice(8, len)
    }
}
```

In any case, the basic idea here is that you describe the ‘shape’ of a table
using standard Rust types, and then add annotations that provide additional
hints as to how the table data should be interpreted. The `CountedSlice`
annotation, for instance, instructs the macro to read a count from the buffer at
this location, and use that as the length of the subsequent field.


## Zero copy and mmap

The crate will not perform any allocation when reading a font, and will avoid
superfluous copying. Non-trivial types like tables will all contain a Buffer
object that provides access to raw bytes, and and this will be passed from
parents to children. Types like arrays are not represented concretely: instead
the Slice type is an abstraction that provides iteration and random access of
members of an array by reading directly from the underlying bytes.

When dealing with scalars or scalar composites, bytes will be read directly and
converted to the appropriate values at access time. Between endianness,
alignment, and other concerns this is a very easy operation to screw up, but
there are two carefully-audited Rust projects for this sort of access (the
[zerocopy](https://crates.io/crates/zerocopy) or
[bytemuck](https://docs.rs/bytemuck/latest/bytemuck/) crates) and for the time
being we will use one of them; in the longer-term we hope that this
functionality will eventually [make it into
std](https://github.com/jswrenn/project-safe-transmute/blob/rfc/rfcs/0000-safe-transmute.md#safe-transmute-rfc).


## Copy-on-write semantics

While zero-copy parsing is necessary for use-cases such as shaping, it does not
cover the case of generating or mutating tables. For this case, all types which
contained a Buffer object would have explicit copy-on-write support. This would
be implemented via the Buffer object itself, which would operate similarly to
the [std::borrow::Cow](http://doc.rust-lang.org/1.57.0/std/borrow/enum.Cow.html)
type, being represented internally as either a shared reference to some bytes or
an owned allocation. This design should allow ownership to be taken of
individual tables (and subtables?) without needing to copy the entire font.

The exact semantics here need more thought. One possibility is that this is less
about ‘copy on write’ than it is about ‘ownership flexibility’, where the same
type could be backed by either owned or unowned data. Alternatively there could
be explicit copy-on-write semantics, where some sort of make_mut method would
ensure that the underlying data was uniquely owned, allocating if necessary.
Which approach makes the most sense will depend heavily on how we want to
support mutation.


## Mutation & creation

This is the biggest question mark for me, but I’ll try and outline my thinking.

The distinction I am making here is between creating new mutable objects and
then adding things to them (which is fairly easy) versus taking existing
objects, creating mutable copies on them, and then modifying them directly
(which feels more complicated).

Creation is easier than mutation: we can generate methods on the various types
that allow the user to pass in owned equivalents, and then serialize them behind
the scenes. For instance: you might have some code like,

```rust
impl Avar<'_> {
    pub fn new(
        version_major: u16,
        version_minor: u16,
        segment_maps: Vec<Vec<AxisValueMap>>,
    ) -> Self { ... }
}
```

Mutation is harder. On the one hand, it would be a tremendous simplification if
we could just avoid it. We can imagine doing this: we just say that the scope of
this crate is limited to IO, and all mutation has to happen at some higher
level, the results of which can then be used to create a new, immutable table.
This feels… unsatisfying, but not catastrophically bad, so let’s consider it our
plan of last resort. What are our other options?

 First, some of our constraints:

- Rust’s strict aliasing rules means that mutation of an underlying buffer
  cannot safely happen through an immutable reference, and we are going to want
  to be using immutable references heavily.
- I believe it is a very valuable invariant of this design is that for every
  type backed by a Buffer, if you were to reinterpret the bytes in the buffer
  you would get an instance identical to the original. To put this another way:
  in the Avar struct in the earlier example, you could not mutate one of the
  parsed fields ( such as version_major) except through some method that also
  directly mutated the buffer. (I’m not 100% convinced this is necessary, but it
  is my current preference)


### Fallible &mut methods

Given this, one simple approach is to have all mutation take place through
methods that will only succeed in the case where the underlying Buffer is owned
data. These could be fallible methods, or it could be documented that these
methods required the data be owned, and we could assert this condition was met
when they’re called. A complication with this method is how it interacts with
subfields: how does the parent pass down a mutable reference to the owned data
in the buffer? It starts to feel like Buffer needs three variants, for Borrowed,
Owned, And BorrowedMut, where the latter woul be basically an &mut reference to
a parent’s owned data. I think this would actually probably work, and isn’t the
worst API.

A possible augmentation of this could use distinct 'mut ref' types; mutating
methods would only be avaiable through these types, and creation of these types
would ensure the uniqueness of the underlying storage:

```rust
// these lifetimes are ugly but you would not need to interact with them during
// normal use.
pub struct AvarMut<'a, 'b>(&'b mut Avar<'a>);

impl Ava

impl<'a> Avar<'a> {
    /// Get mutable access
    pub fn as_mut<'b>(&mut self) -> AvarMut<'a, 'b> where 'a: 'b {
        self.buffer.ensure_mut();
        AvarMut(self)
    }
}
```

This would be similar to the pattern above, except that the method which created
the Mut variant of the type would ensure that the buffer was owned, and would
also ensure that the table’s scalar values were correct, etc.


### Distinct owned types

The last alternative I would consider is to have completely distinct types for
the owned case, and to have the owned types not be backed by a buffer at all,
but to instead have conversion methods between the two types.

The advantage of this is that the owned variants are fairly simple rust types:
instead of some Slice<AxisValueMap> that is a view into raw bytes and has its
own API,  you end up with a Vec<AxisValueMap> that you can use like a normal
Rust Vec.

The downside of this approach is that to make it work well, there is going to be
some type complexity. Not only are you going to need Avar and AvarOwned, you are
also going to want some set of traits for representing the relationships between
them, and I’m not sure how complicated that would end up being.

In any case, I think that these three approaches are all interesting, and I
would like to spend some time exploring them; but I suspect there is a
reasonable solution somewhere in this space.


## Extensibility

A final element of the design I would like to highlight is extensibility. The
basic idea here is that tables would not need to all be defined in a single
canonical place. If a user wanted to add support for a new, non-standard table,
all of the macros required to describe it would be available publicly, and they
would only need to write out and annotate the structs.


# Prior art

There are a number of existing projects that I think are relevant to this work,
and which encourage me as to its feasibility. The first of these is
[fonttools-rs](https://github.com/simoncozens/fonttools-rs), and its components
otspec and otspec-macros. This is a set of crates by Simon Cozens that are
focused on the compilation case (data is stored in native rust types and
converted to raw bytes during serialization and deserialization) but it uses a
system of macros to generate that IO code, to good effect. The second of these
is Chad Brokow’s [pinot](https://github.com/dfrg/pinot), which is an existing
zero-copy OpenType parser intended to be used for shaping.

Between these two projects, the majority of the goals i have for this project
have already been demonstrated to be feasible; the challenge here will be
largely to figure out a way to combine them in a sensible and ergonomic way.

Outside of Rust, I think it will also be important to spend some quality time
with HarfBuzz, to ensure that we are able to


# Schedule and time allocation

The initial goal for this work is to determine whether it is feasible at all.
With that in mind, I think we can break the project up into three phases, where
we can abandon the work if any particular phase is a failure:


1. macros, zero-copy types: demonstrate that we can use macros to generate code
   for interpreting and accessing values from a small handful of tables,
   including some tricky tables like GPOS/GSUB. 4-6 weeks.
2. mutability: decide on a strategy for mutability, and implement it for that
   same handful of tables. 6-10 weeks. It is possible that as we work on
   mutability we will decide that we need to revisit how we structure our
   code-gen code: for instance if we need to generate multiple types for each
   table, a procedural macro may not be the best choice.
3. Implement all core tables: ideally this is mostly just a matter of typing,
   but it might still be quite a lot of typing. I’ll feel more comfortable
   giving this a time estimate after I’ve finished the previous items, but if I
   had to guess I would say ~12 weeks.


# Questions

There are a few remaining things I am currently unsure about:

- how should the library handle errors during parsing? Is there any utility in
  reporting the locations of errors? Do we assume good data? If we try to handle
  errors ‘correctly’ then likely the entire API surface will need to return
  Result types, and that might get exhausting.
- is there a DRY method for generating methods that access various fields on
  various tables? How much variation is there likely to be in terms of the API
  requirements of different tables?
- How extensively do we want to be using concrete types, in place of things like
  type aliases/raw scalars? For instance a u16 can be used to represent a large
  number of possible things in the OpenType spec, from a glyph identifier to an
  F2DOT14 to an Offset16, an FWORD, etc etc. My inclination would be to have
  newtypes for all distinct data types, but that might seem cumbersome to some
  folks.
- Certain collections are hetergenous, such as the different subtable types in a
  layout table. Can we generate nice code that represents these types using
  enums where appropriate?

